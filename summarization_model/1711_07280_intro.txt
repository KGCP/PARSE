The idea that we might be able to give general, verbal
instructions to a robot and have at least a reasonable probability
that it will carry out the required task is one of the
long-held goals of robotics, and artificial intelligence (AI).
Despite significant progress, there are a number of major
technical challenges that need to be overcome before robots
will be able to perform general tasks in the real world. One
of the primary requirements will be new techniques for linking
natural language to vision and action in unstructured,
previously unseen environments. It is the navigation version of this challenge that we refer to as Vision-and-Language
Navigation (VLN).
Although interpreting natural-language navigation instructions
has received significant attention previously [12,
13, 20, 38, 41, 52], it is the recent success of recurrent neural
network methods for the joint interpretation of images
and natural language that motivates the VLN task, and the
associated Room-to-Room (R2R) dataset described below.
The dataset particularly has been designed to simplify the
application of vision and language methods to what might
otherwise seem a distant problem.
Previous approaches to natural language command of
robots have often neglected the visual information processing
aspect of the problem. Using rendered, rather than real
images [7, 27, 62], for example, constrains the set of visible objects to the set of hand-crafted models available to
the renderer. This turns the robotâ€™s challenging open-set
problem of relating real language to real imagery into a far
simpler closed-set classification problem. The natural extension
of this process is that adopted in works where the
images are replaced by a set of labels [13, 52]. Limiting
the variation in the imagery inevitably limits the variation
in the navigation instructions also. What distinguishes the
VLN challenge is that the agent is required to interpret a
previously unseen natural-language navigation command in
light of images generated by a previously unseen real environment.
The task thus more closely models the distinctly
open-set nature of the underlying problem.
To enable the reproducible evaluation of VLN methods,
we present the Matterport3D Simulator. The simulator is a
large-scale interactive reinforcement learning (RL) environment
constructed from the Matterport3D dataset [11] which
contains 10,800 densely-sampled panoramic RGB-D images
of 90 real-world building-scale indoor environments.
Compared to synthetic RL environments [7, 27, 62], the
use of real-world image data preserves visual and linguistic
richness, maximizing the potential for trained agents to
be transferred to real-world applications.
Based on the Matterport3D environments, we collect
the Room-to-Room (R2R) dataset containing 21,567 openvocabulary,
crowd-sourced navigation instructions with an
average length of 29 words. Each instruction describes a
trajectory traversing typically multiple rooms. As illustrated
in Figure 1, the associated task requires an agent to
follow natural-language instructions to navigate to a goal
location in a previously unseen building. We investigate the
difficulty of this task, and particularly the difficulty of opoperating
in unseen environments, using several baselines and
a sequence-to-sequence model based on methods successfully
applied to other vision and language tasks [4, 14, 19].
In summary, our main contributions are: 1. We introduce the Matterport3D Simulator, a software
framework for visual reinforcement learning using the Matterport3D panoramic RGB-D dataset [11];
2. We present Room-to-Room (R2R), the first benchmark
dataset for Vision-and-Language Navigation in real,
previously unseen, building-scale 3D environments;
3. We apply sequence-to-sequence neural networks to the
R2R dataset, establishing several baselines.
The simulator, R2R dataset and baseline models
are available through the project website at
https://bringmeaspoon.org


researchers present a new way of linking natural language to vision and action in unstructured, previously unseen environments. They present the first benchmark dataset for interpreting natural-language navigation instructions in a real world environment. They also present a simulator for visual reinforcement learning using the Matterport3D dataset.
